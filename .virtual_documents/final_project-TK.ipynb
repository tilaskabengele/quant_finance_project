





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from hmmlearn.hmm import GaussianHMM
import yfinance as yf

def fetch_data(ticker, start_date, end_date):
    """
    Fetch historical OHLC data for a given ticker from Yahoo Finance.
    
    Args:
        ticker (str): The stock/ETF ticker symbol (e.g., 'SPY' for S&P 500 ETF).
        start_date (str): The start date for data in the format 'YYYY-MM-DD'.
        end_date (str): The end date for data in the format 'YYYY-MM-DD'.
        
    Returns:
        DataFrame: Historical OHLCV data.
    """
    data = yf.download(ticker, start=start_date, end=end_date)
    return data

# Fetching S&P500 ETF data ('SPY') from Yahoo Finance
start_date = "2007-01-01"
end_date = "2020-04-01"
ticker = "SPY"

data = fetch_data(ticker, start_date, end_date)

# Display the first few rows of the data
print(data.head())



# Plotting the closing price
plt.figure(figsize=(12, 6))
plt.plot(data['Close'], label=f'{ticker} Closing Price')
plt.title(f'{ticker} Closing Price ({start_date} to {end_date})')
plt.xlabel('Date')
plt.ylabel('Price (USD)')
plt.legend()
plt.show()






def preprocess_data(data):
    """
    Preprocess the data to calculate daily returns and volatility.

    Args:
        data (DataFrame): Historical OHLC data.

    Returns:
        DataFrame: Preprocessed data with daily returns and volatility.
    """
    # Calculate daily returns
    data['Daily_Return'] = data['Adj Close'].pct_change()

    # Calculate rolling volatility (10-day rolling standard deviation of returns)
    data['Volatility'] = data['Daily_Return'].rolling(window=10).std()

    # Drop rows with NaN values resulting from rolling calculations
    data = data.dropna()

    return data

# Preprocess the data
preprocessed_data = preprocess_data(data)

# Display the first few rows of the preprocessed data
print(preprocessed_data.head())



# Plotting daily returns
plt.figure(figsize=(12, 6))
plt.plot(preprocessed_data['Daily_Return'], label='Daily Return', alpha=0.7)
plt.title('Daily Returns')
plt.xlabel('Date')
plt.ylabel('Daily Return')
plt.legend()
plt.show()

# Plotting volatility
plt.figure(figsize=(12, 6))
plt.plot(preprocessed_data['Volatility'], label='10-Day Rolling Volatility', alpha=0.7, color='orange')
plt.title('Volatility')
plt.xlabel('Date')
plt.ylabel('Volatility')
plt.legend()
plt.show()



# Split the data into training and testing sets
split_date = '2017-09-01'
train_data = preprocessed_data.loc[:split_date]
test_data = preprocessed_data.loc[split_date:]

# Visualize Train/Test Split
plt.figure(figsize=(12, 6))
plt.plot(preprocessed_data['Daily_Return'], label="Daily Returns", alpha=0.6)
plt.axvline(pd.Timestamp(split_date), color='red', linestyle='--', label="Train/Test Split")
plt.title("Train/Test Split Visualization")
plt.xlabel("Date")
plt.ylabel("Daily Return")
plt.legend()
plt.show()

# Display training and testing period
print(f"Training Period: {train_data.index.min()} to {train_data.index.max()}")
print(f"Validation Period: {test_data.index.min()} to {test_data.index.max()}")









from sklearn.preprocessing import StandardScaler

def train_hmm(data, n_states=3):
    """
    Train a Hidden Markov Model (HMM) on the training dataset.
    
    Args:
        data (DataFrame): Training data containing features for HMM.
        n_states (int): Number of hidden states (e.g., bull, bear, neutral).
        
    Returns:
        model: Trained HMM model.
        np.ndarray: Scaled feature array used for training.
        scaler: StandardScaler object used for scaling.
    """
    # Extract features (Daily Return and Volatility)
    features = data[['Daily_Return', 'Volatility']].values

    # Standardize features
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)

    # Train Gaussian HMM
    hmm_model = GaussianHMM(n_components=n_states, covariance_type="full", n_iter=1000, random_state=42)
    hmm_model.fit(features_scaled)

    return hmm_model, features_scaled, scaler

# Train the HMM
n_states = 3
hmm_model, train_features_scaled, scaler = train_hmm(train_data, n_states=n_states)

# Analyze Model Parameters
print("HMM Parameters:")
print("Transition Matrix (A):")
print(hmm_model.transmat_)
print("Means (μ):")
print(hmm_model.means_)
print("Covariances (σ^2):")
print(hmm_model.covars_)
print("Initial State Probabilities (π):")
print(hmm_model.startprob_)

# Evaluate In-Sample Log-Likelihood
in_sample_log_likelihood = hmm_model.score(train_features_scaled)
print(f"In-Sample Log-Likelihood: {in_sample_log_likelihood}")



def validate_hmm(hmm_model, data, scaler):
    """
    Validate the HMM on the test dataset.
    
    Args:
        hmm_model: Trained HMM model.
        data (DataFrame): Test data for validation.
        scaler: StandardScaler used for scaling the training data.
        
    Returns:
        DataFrame: Test data with predicted regimes.
        float: Log-likelihood of the test data.
    """
    # Extract and scale features
    features = data[['Daily_Return', 'Volatility']].values
    features_scaled = scaler.transform(features)

    # Predict regimes
    regimes = hmm_model.predict(features_scaled)

    # Compute log-likelihood
    log_likelihood = hmm_model.score(features_scaled)

    # Add predicted regimes to the test data
    validated_data = data.copy()
    validated_data['Regime'] = regimes

    return validated_data, log_likelihood

# Validate the HMM on test data
validated_test_data, out_sample_log_likelihood = validate_hmm(hmm_model, test_data, scaler)

# Display results
print(f"Out-of-Sample Log-Likelihood: {out_sample_log_likelihood}")
print(validated_test_data[['Daily_Return', 'Volatility', 'Regime']].head())



# Add predicted regimes to training data
train_data = train_data.copy()
train_data['Regime'] = hmm_model.predict(train_features_scaled)

# Plot training data regimes
plt.figure(figsize=(12, 6))
for regime in range(n_states):
    regime_data = train_data[train_data['Regime'] == regime]
    plt.scatter(regime_data.index, regime_data['Daily_Return'], label=f"Regime {regime}", alpha=0.6)

plt.title("In-Sample Regime Classification")
plt.xlabel("Date")
plt.ylabel("Daily Return")
plt.legend()
plt.show()



# Plot test data regimes
plt.figure(figsize=(12, 6))
for regime in range(n_states):
    regime_data = validated_test_data[validated_test_data['Regime'] == regime]
    plt.scatter(regime_data.index, regime_data['Daily_Return'], label=f"Regime {regime}", alpha=0.6)

plt.title("Out-of-Sample Regime Classification")
plt.xlabel("Date")
plt.ylabel("Daily Return")
plt.legend()
plt.show()






# Flatten MultiIndex columns
train_data.columns = ['_'.join(col).strip() for col in train_data.columns.values]
validated_test_data.columns = ['_'.join(col).strip() for col in validated_test_data.columns.values]

# Confirm column names
print("Flattened columns in train_data:", train_data.columns)
print("Flattened columns in validated_test_data:", validated_test_data.columns)



# Compute statistics for the training data
train_regime_stats = train_data.groupby('Regime_')[['Daily_Return_', 'Volatility_']].mean()
print("In-Sample Regime Statistics (Training Data):")
print(train_regime_stats)



# Compute statistics for the test data
test_regime_stats = validated_test_data.groupby('Regime_')[['Daily_Return_', 'Volatility_']].mean()
print("Out-of-Sample Regime Statistics (Test Data):")
print(test_regime_stats)



def evaluate_log_likelihood(data, max_states=5):
    """
    Evaluate log-likelihood for different numbers of hidden states.
    
    Args:
        data (DataFrame): Training dataset.
        max_states (int): Maximum number of hidden states to evaluate.
    
    Returns:
        list: Log-likelihood values for each number of states.
    """
    features = data[['Daily_Return_', 'Volatility_']].values
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)

    log_likelihoods = []
    for n_states in range(2, max_states + 1):
        hmm_model = GaussianHMM(n_components=n_states, covariance_type="full", n_iter=1000, random_state=42)
        hmm_model.fit(features_scaled)
        log_likelihood = hmm_model.score(features_scaled)
        log_likelihoods.append(log_likelihood)
        print(f"Log-Likelihood for {n_states} states: {log_likelihood}")
    
    return log_likelihoods

# Evaluate log-likelihoods for 2 to 10 states
log_likelihoods = evaluate_log_likelihood(train_data, max_states=10)

# Plot log-likelihoods
plt.figure(figsize=(10, 6))
plt.plot(range(2, 11), log_likelihoods, marker='o', label="Log-Likelihood")
plt.title("Log-Likelihood vs. Number of Hidden States")
plt.xlabel("Number of Hidden States")
plt.ylabel("Log-Likelihood")
plt.grid(True)
plt.legend()
plt.show()



# Combine train and test data
full_data = pd.concat([train_data, validated_test_data])

# Ensure regimes are classified for the entire dataset
features = full_data[['Daily_Return_', 'Volatility_']].values
features_scaled = scaler.transform(features)
full_data['Regime_'] = hmm_model.predict(features_scaled)



import matplotlib.pyplot as plt

# Ensure closing prices are forward-filled
full_data['Close_SPY'] = full_data['Close_SPY'].ffill()

# Create the combined figure
fig, axes = plt.subplots(2, 1, figsize=(15, 12), sharex=True)

# Figure 4a: Regime classification over time
for regime in range(n_states):
    regime_data = full_data[full_data['Regime_'] == regime]
    axes[0].plot(regime_data.index, regime_data['Regime_'], '.', label=f'Regime {regime}', alpha=0.6)
axes[0].axvline(pd.Timestamp(split_date), color='red', linestyle='--', label="Train/Test Split")
axes[0].set_title("Time-Series of Regime Classification")
axes[0].set_ylabel("Regime")
axes[0].legend()
axes[0].grid(True)

# Figure 4b: Closing price with regime classification and train/test split
for regime in range(n_states):
    regime_data = full_data[full_data['Regime_'] == regime]
    axes[1].plot(regime_data.index, regime_data['Close_SPY'], '.', label=f'Regime {regime}', alpha=0.6)
axes[1].plot(full_data.index, full_data['Close_SPY'], color='black', alpha=0.4, label="Closing Price")
axes[1].axvline(pd.Timestamp(split_date), color='red', linestyle='--', label="Train/Test Split")
axes[1].set_title("Closing Price with Regime Classification")
axes[1].set_xlabel("Date")
axes[1].set_ylabel("Price (USD)")
axes[1].legend()
axes[1].grid(True)

# Show the combined figure
plt.tight_layout()
plt.show()



from sklearn.metrics import silhouette_score 

# Ensure features are scaled for the entire dataset (used in 3-state model)
features_scaled_3 = scaler.transform(full_data[['Daily_Return_', 'Volatility_']].values)

# Compute silhouette score for 3-state model
silhouette_score_3 = silhouette_score(features_scaled_3, full_data['Regime_'])
print(f"Silhouette Score (3-State Model): {silhouette_score_3}")






# Flatten test_data columns
test_data.columns = ['_'.join(col).strip() for col in test_data.columns.values]

# Confirm column names
print("Flattened test_data columns:", test_data.columns)



# Train the HMM with 5 states
def train_hmm_5(data, n_states=5):
    """
    Train a Hidden Markov Model (HMM) with 5 states.
    
    Args:
        data (DataFrame): Training data containing features for HMM.
        n_states (int): Number of hidden states.
        
    Returns:
        model: Trained HMM model.
        scaler: StandardScaler object used for scaling.
        features_scaled: Scaled feature array used for training.
    """
    # Extract features
    features = data[['Daily_Return_', 'Volatility_']].values

    # Standardize features
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)

    # Train the HMM
    hmm_model = GaussianHMM(n_components=n_states, covariance_type="full", n_iter=1000, random_state=42)
    hmm_model.fit(features_scaled)

    return hmm_model, scaler, features_scaled

# Train the 5-state HMM
hmm_model_5, scaler_5, train_features_scaled_5 = train_hmm_5(train_data)



# Add Regime_ column to train_data
train_data = train_data.copy()
train_data['Regime_'] = hmm_model_5.predict(train_features_scaled_5)

# Add Regime_ column to test_data
test_data = test_data.copy()
test_features = test_data[['Daily_Return_', 'Volatility_']].values
test_features_scaled = scaler_5.transform(test_features)
test_data['Regime_'] = hmm_model_5.predict(test_features_scaled)

# Combine train_data and test_data
full_data_5 = pd.concat([train_data, test_data])



train_regime_stats_5 = train_data.groupby('Regime_')[['Daily_Return_', 'Volatility_']].mean()
test_regime_stats_5 = test_data.groupby('Regime_')[['Daily_Return_', 'Volatility_']].mean()

print("In-Sample Regime Statistics (5 States):")
print(train_regime_stats_5)
print("Out-of-Sample Regime Statistics (5 States):")
print(test_regime_stats_5)



import matplotlib.pyplot as plt

# Create the combined figure
fig, axes = plt.subplots(2, 1, figsize=(15, 12), sharex=True)

# Plot 1: Time-Series of Regime Classification
for regime in range(5):
    regime_data = full_data_5[full_data_5['Regime_'] == regime]
    axes[0].plot(regime_data.index, regime_data['Regime_'], '.', label=f'Regime {regime}', alpha=0.6)
axes[0].axvline(pd.Timestamp(split_date), color='red', linestyle='--', label="Train/Test Split")
axes[0].set_title("Time-Series of Regime Classification (5 States)")
axes[0].set_ylabel("Regime")
axes[0].legend()
axes[0].grid(True)

# Plot 2: Closing Price with Regime Classification
for regime in range(5):
    regime_data = full_data_5[full_data_5['Regime_'] == regime]
    axes[1].plot(regime_data.index, regime_data['Close_SPY'], '.', label=f'Regime {regime}', alpha=0.6)
axes[1].plot(full_data_5.index, full_data_5['Close_SPY'], color='black', alpha=0.4, label="Closing Price")
axes[1].axvline(pd.Timestamp(split_date), color='red', linestyle='--', label="Train/Test Split")
axes[1].set_title("Closing Price with Regime Classification (5 States)")
axes[1].set_xlabel("Date")
axes[1].set_ylabel("Price (USD)")
axes[1].legend()
axes[1].grid(True)

# Display the combined plot
plt.tight_layout()
plt.show()



print("In-Sample Regime Statistics (5 States):")
print(train_regime_stats_5)

print("\nOut-of-Sample Regime Statistics (5 States):")
print(test_regime_stats_5)



print("Transition Matrix (5-State HMM):")
print(hmm_model_5.transmat_)



from sklearn.metrics import silhouette_score

# Compute silhouette score for the regimes
features_scaled = scaler_5.transform(full_data_5[['Daily_Return_', 'Volatility_']].values)
silhouette = silhouette_score(features_scaled, full_data_5['Regime_'])
print(f"Silhouette Score (Regime Separation): {silhouette}")






# Feature space for 3-state and 5-state models
fig, axes = plt.subplots(2, 1, figsize=(12, 12), sharex=True, sharey=True)

# Plot for 3-state HMM
for regime in range(3):
    regime_data_3 = full_data[full_data['Regime_'] == regime]
    axes[0].scatter(regime_data_3['Volatility_'], regime_data_3['Daily_Return_'], label=f'Regime {regime}', alpha=0.6)
axes[0].set_title("Feature Space: Daily Return vs. Volatility (3-State HMM)")
axes[0].set_xlabel("Volatility")
axes[0].set_ylabel("Daily Return")
axes[0].legend()
axes[0].grid(True)

# Plot for 5-state HMM
for regime in range(5):
    regime_data_5 = full_data_5[full_data_5['Regime_'] == regime]
    axes[1].scatter(regime_data_5['Volatility_'], regime_data_5['Daily_Return_'], label=f'Regime {regime}', alpha=0.6)
axes[1].set_title("Feature Space: Daily Return vs. Volatility (5-State HMM)")
axes[1].set_xlabel("Volatility")
axes[1].set_ylabel("Daily Return")
axes[1].legend()
axes[1].grid(True)

# Adjust layout and display
plt.tight_layout()
plt.show()









from hmmlearn.hmm import GaussianHMM
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import pandas as pd

# --- Define functions ---
def train_hmm(data, n_states=3):
    """
    Train a Hidden Markov Model (HMM) on the training dataset.
    """
    # Extract features
    features = data[['Daily_Return_', 'Volatility_']].values

    # Standardize features
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)

    # Train Gaussian HMM
    hmm_model = GaussianHMM(n_components=n_states, covariance_type="full", n_iter=1000, random_state=42)
    hmm_model.fit(features_scaled)

    return hmm_model, features_scaled, scaler

def validate_hmm(hmm_model, data, scaler):
    """
    Validate the HMM on the test dataset.
    """
    # Extract and scale features
    features = data[['Daily_Return_', 'Volatility_']].values
    features_scaled = scaler.transform(features)

    # Predict regimes
    regimes = hmm_model.predict(features_scaled)

    # Compute log-likelihood
    log_likelihood = hmm_model.score(features_scaled)

    # Add predicted regimes to the test data
    validated_data = data.copy()
    validated_data['Regime_'] = regimes

    return validated_data, log_likelihood

# --- Split data consistently ---
split_date = "2017-09-01"
train_data = full_data.loc[:split_date]
test_data = full_data.loc[split_date:]

# --- Train the HMM ---
n_states = 3
hmm_model, train_features_scaled, scaler = train_hmm(train_data, n_states=n_states)

# --- Validate the HMM ---
validated_test_data, out_sample_log_likelihood = validate_hmm(hmm_model, test_data, scaler)

# --- Print Log-Likelihood Results ---
print(f"In-Sample Log-Likelihood: {hmm_model.score(train_features_scaled)}")
print(f"Out-of-Sample Log-Likelihood: {out_sample_log_likelihood}")

# --- Add predicted regimes to training data ---
# Add predicted regimes to training data
train_data = train_data.copy()  # Avoid SettingWithCopyWarning
train_data['Regime_'] = hmm_model.predict(train_features_scaled)

# Compute Regime-Specific Statistics
train_regime_stats = train_data.groupby('Regime_')[['Daily_Return_', 'Volatility_']].mean()
test_regime_stats = validated_test_data.groupby('Regime_')[['Daily_Return_', 'Volatility_']].mean()

print("In-Sample Regime Statistics (Training Data):")
print(train_regime_stats)
print("Out-of-Sample Regime Statistics (Test Data):")
print(test_regime_stats)


# --- Plot Regime Classifications ---
def plot_regime_classification(data, title):
    plt.figure(figsize=(12, 6))
    for regime in range(n_states):
        regime_data = data[data['Regime_'] == regime]
        plt.scatter(regime_data.index, regime_data['Daily_Return_'], label=f"Regime {regime}", alpha=0.6)
    plt.title(title)
    plt.xlabel("Date")
    plt.ylabel("Daily Return")
    plt.legend()
    plt.show()

# Plot training data regimes
plot_regime_classification(train_data, "In-Sample Regime Classification")

# Plot test data regimes
plot_regime_classification(validated_test_data, "Out-of-Sample Regime Classification")









import yfinance as yf
import pandas as pd
import numpy as np
from hmmlearn.hmm import GaussianHMM
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# --- Data Loading and Preprocessing ---
def load_sector_data(tickers, start_date, end_date):
    data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']
    returns = data.pct_change().dropna()
    return data, returns

# --- HMM Training and Validation ---
def train_hmm(data, n_states=3):
    features = data[['Daily_Return', 'Volatility']].values
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    hmm_model = GaussianHMM(n_components=n_states, covariance_type="full", n_iter=1000, random_state=42)
    hmm_model.fit(features_scaled)
    return hmm_model, scaler

def validate_hmm(hmm_model, data, scaler):
    features = data[['Daily_Return', 'Volatility']].values
    features_scaled = scaler.transform(features)
    regimes = hmm_model.predict(features_scaled)
    log_likelihood = hmm_model.score(features_scaled)
    validated_data = data.copy()
    validated_data['Regime'] = regimes
    return validated_data, log_likelihood

# --- Portfolio Optimization ---
def calculate_weights(performance):
    performance += abs(performance.min()) + 1e-6
    return performance / performance.sum()

def compute_portfolio_returns(sector_returns, weights_by_regime, tickers):
    portfolio_returns = []
    for date, row in sector_returns.iterrows():
        try:
            regime = row['Regime']
            weights = weights_by_regime.loc[regime]
            daily_return = sum(weights[ticker] * row[ticker] for ticker in tickers)
            portfolio_returns.append(daily_return)
        except KeyError:
            portfolio_returns.append(0)
    return portfolio_returns

# --- Metrics Calculation ---
def calculate_sharpe_ratio(portfolio_returns, risk_free_rate=0.02):
    excess_returns = portfolio_returns - risk_free_rate / 252
    return np.mean(excess_returns) / np.std(excess_returns)

def calculate_max_drawdown(cumulative_returns):
    roll_max = cumulative_returns.cummax()
    drawdown = (cumulative_returns - roll_max) / roll_max
    return drawdown.min()

# --- Visualization ---
def plot_cumulative_returns(data, title):
    plt.figure(figsize=(12, 6))
    plt.plot(data['Cumulative_Portfolio_Return'], label='Regime-Based Portfolio Return')
    plt.plot(data['Cumulative_Equal_Weighted_Return'], label='Equal-Weighted Portfolio Return', linestyle='--')
    plt.title(title)
    plt.xlabel("Date")
    plt.ylabel("Cumulative Return")
    plt.legend()
    plt.yscale('log')
    plt.show()

def plot_performance_by_regime(data, tickers, title):
    plt.figure(figsize=(12, 6))
    regimes = data['Regime'].unique()
    for regime in regimes:
        regime_data = data[data['Regime'] == regime]
        for ticker in tickers:
            plt.scatter(regime_data.index, regime_data[ticker], label=f"Regime {regime}: {ticker}", alpha=0.6)
    plt.title(title)
    plt.xlabel("Date")
    plt.ylabel("Daily Return")
    plt.legend()
    plt.show()


def plot_regime_distribution(data, title):
    """
    Plot the distribution of regimes over time.
    """
    regime_counts = data['Regime'].value_counts()
    regime_counts.plot(kind='bar', figsize=(8, 4), title=title, color='skyblue')
    plt.xlabel("Regime")
    plt.ylabel("Frequency")
    plt.show()
    
# --- Main Execution ---
if __name__ == "__main__":
    # Parameters
    sector_tickers = ['XLU', 'XLK']
    start_date = "2007-01-01"
    end_date = "2020-03-31"
    split_date = "2017-09-01"
    n_states = 3

    # Load and preprocess data
    sector_data, sector_returns = load_sector_data(sector_tickers, start_date, end_date)
    full_data = sector_returns.mean(axis=1).to_frame(name='Daily_Return')
    full_data['Volatility'] = sector_returns.std(axis=1)

    # Train-test split
    train_data = full_data.loc[:split_date].copy()
    test_data = full_data.loc[split_date:].copy()

    # Train and validate HMM
    hmm_model, scaler = train_hmm(train_data, n_states)
    validated_test_data, out_sample_log_likelihood = validate_hmm(hmm_model, test_data, scaler)

    # Assign regimes
    full_data['Regime'] = hmm_model.predict(scaler.transform(full_data[['Daily_Return', 'Volatility']].values))
    sector_returns['Regime'] = full_data['Regime']

    # Analyze regime statistics
    sector_performance_by_regime = sector_returns.groupby('Regime').mean()
    weights_by_regime = sector_performance_by_regime.apply(calculate_weights, axis=1)

    # Compute portfolio returns
    sector_returns['Portfolio_Return'] = compute_portfolio_returns(sector_returns, weights_by_regime, sector_tickers)
    sector_returns['Cumulative_Portfolio_Return'] = (1 + sector_returns['Portfolio_Return']).cumprod()
    sector_returns['Equal_Weighted_Return'] = sector_returns[sector_tickers].mean(axis=1)
    sector_returns['Cumulative_Equal_Weighted_Return'] = (1 + sector_returns['Equal_Weighted_Return']).cumprod()

    # Metrics
    sharpe_ratio_portfolio = calculate_sharpe_ratio(sector_returns['Portfolio_Return'])
    sharpe_ratio_equal = calculate_sharpe_ratio(sector_returns['Equal_Weighted_Return'])
    max_drawdown_portfolio = calculate_max_drawdown(sector_returns['Cumulative_Portfolio_Return'])
    max_drawdown_equal = calculate_max_drawdown(sector_returns['Cumulative_Equal_Weighted_Return'])

    print(f"Sharpe Ratio - Regime-Based Portfolio: {sharpe_ratio_portfolio}")
    print(f"Sharpe Ratio - Equal-Weighted Portfolio: {sharpe_ratio_equal}")
    print(f"Max Drawdown - Regime-Based Portfolio: {max_drawdown_portfolio}")
    print(f"Max Drawdown - Equal-Weighted Portfolio: {max_drawdown_equal}")

    # Visualizations
    plot_cumulative_returns(sector_returns, "Portfolio Performance: Regime-Based vs Equal-Weighted")
    plot_performance_by_regime(sector_returns.loc[train_data.index], sector_tickers, "Train Data: Sector Performance by Regime")
    plot_performance_by_regime(sector_returns.loc[test_data.index], sector_tickers, "Test Data: Sector Performance by Regime")
    plot_regime_distribution(full_data, "Regime Distribution Over Time")






# Parameters
sector_tickers = ['XLK', 'XLU', 'XLF', 'XLE', 'XLV']
start_date = "2007-01-01"
end_date = "2020-03-31"
split_date = "2017-09-01"
n_states = 3

# Load and preprocess data
sector_data, sector_returns = load_sector_data(sector_tickers, start_date, end_date)
full_data = sector_returns.mean(axis=1).to_frame(name='Daily_Return')
full_data['Volatility'] = sector_returns.std(axis=1)

# Split data into training and testing
train_data = full_data.loc[:split_date].copy()
test_data = full_data.loc[split_date:].copy()

# Train and validate HMM
hmm_model, scaler = train_hmm(train_data, n_states)
validated_test_data, out_sample_log_likelihood = validate_hmm(hmm_model, test_data, scaler)

# Assign regimes to full dataset
full_data['Regime'] = hmm_model.predict(scaler.transform(full_data[['Daily_Return', 'Volatility']].values))
sector_returns['Regime'] = full_data['Regime']

# Analyze regime statistics
sector_performance_by_regime = sector_returns.groupby('Regime').mean()
weights_by_regime = sector_performance_by_regime.apply(calculate_weights, axis=1)

# Compute portfolio returns
sector_returns['Portfolio_Return'] = compute_portfolio_returns(sector_returns, weights_by_regime, sector_tickers)
sector_returns['Cumulative_Portfolio_Return'] = (1 + sector_returns['Portfolio_Return']).cumprod()
sector_returns['Equal_Weighted_Return'] = sector_returns[sector_tickers].mean(axis=1)
sector_returns['Cumulative_Equal_Weighted_Return'] = (1 + sector_returns['Equal_Weighted_Return']).cumprod()

# Visualizations
plot_cumulative_returns(sector_returns, "Portfolio Performance: Regime-Based vs Equal-Weighted")
plot_performance_by_regime(sector_returns.loc[train_data.index], sector_tickers, "Train Data: Sector Performance by Regime")
plot_performance_by_regime(sector_returns.loc[test_data.index], sector_tickers, "Test Data: Sector Performance by Regime")
plot_regime_distribution(full_data, "Regime Distribution Over Time")

# Evaluate performance metrics
sharpe_regime = (sector_returns['Portfolio_Return'].mean() / sector_returns['Portfolio_Return'].std()) * np.sqrt(252)
sharpe_equal = (sector_returns['Equal_Weighted_Return'].mean() / sector_returns['Equal_Weighted_Return'].std()) * np.sqrt(252)

print(f"Sharpe Ratio - Regime-Based Portfolio: {sharpe_regime}")
print(f"Sharpe Ratio - Equal-Weighted Portfolio: {sharpe_equal}")

def calculate_max_drawdown(cumulative_returns):
    """
    Calculate the maximum drawdown of a portfolio.
    """
    peak = cumulative_returns.expanding(min_periods=1).max()
    drawdown = (cumulative_returns - peak) / peak
    return drawdown.min()

max_drawdown_regime = calculate_max_drawdown(sector_returns['Cumulative_Portfolio_Return'])
max_drawdown_equal = calculate_max_drawdown(sector_returns['Cumulative_Equal_Weighted_Return'])

print(f"Max Drawdown - Regime-Based Portfolio: {max_drawdown_regime}")
print(f"Max Drawdown - Equal-Weighted Portfolio: {max_drawdown_equal}")






# --- Rolling Sharpe Ratio ---
def compute_rolling_sharpe(returns, window=252):
    """
    Compute rolling Sharpe Ratio.
    """
    rolling_mean = returns.rolling(window=window).mean()
    rolling_std = returns.rolling(window=window).std()
    rolling_sharpe = rolling_mean / rolling_std
    return rolling_sharpe

# --- Sortino Ratio ---
def compute_sortino_ratio(returns, target=0):
    """
    Compute Sortino Ratio for a given return series.
    """
    excess_return = returns - target
    downside_std = np.sqrt(np.mean(np.minimum(0, excess_return) ** 2))
    sortino_ratio = np.mean(excess_return) / downside_std
    return sortino_ratio

# --- Calmar Ratio ---
def compute_calmar_ratio(returns, max_drawdown):
    """
    Compute Calmar Ratio for a given return series.
    """
    annualized_return = np.mean(returns) * 252
    return annualized_return / abs(max_drawdown)

# --- Stress Testing ---
def backtest_by_subperiods(data, n_splits=3):
    """
    Divide data into sub-periods and compute Sharpe, Sortino, and Max Drawdown for each.
    """
    split_points = np.linspace(0, len(data), n_splits + 1, dtype=int)
    subperiod_results = []
    for i in range(n_splits):
        start, end = split_points[i], split_points[i + 1]
        sub_data = data.iloc[start:end]
        
        # Metrics for the subperiod
        sharpe_ratio = compute_rolling_sharpe(sub_data['Portfolio_Return']).mean()
        sortino_ratio = compute_sortino_ratio(sub_data['Portfolio_Return'])
        max_drawdown = compute_max_drawdown(sub_data['Cumulative_Portfolio_Return'])
        subperiod_results.append({'Sharpe': sharpe_ratio, 'Sortino': sortino_ratio, 'Max_Drawdown': max_drawdown})
    
    return pd.DataFrame(subperiod_results)


def compute_max_drawdown(cumulative_returns):
    """
    Compute the maximum drawdown for a given cumulative return series.
    """
    rolling_max = cumulative_returns.cummax()
    drawdown = cumulative_returns / rolling_max - 1
    max_drawdown = drawdown.min()
    return max_drawdown

def plot_rolling_metrics(portfolio, equal_weighted, title):
    """
    Plot rolling Sharpe ratios for both portfolios.
    """
    rolling_sharpe_portfolio = compute_rolling_sharpe(portfolio['Portfolio_Return'])
    rolling_sharpe_equal = compute_rolling_sharpe(portfolio['Equal_Weighted_Return'])
    
    plt.figure(figsize=(12, 6))
    plt.plot(rolling_sharpe_portfolio, label='Regime-Based Portfolio')
    plt.plot(rolling_sharpe_equal, label='Equal-Weighted Portfolio', linestyle='--')
    plt.title(title)
    plt.xlabel("Date")
    plt.ylabel("Rolling Sharpe Ratio")
    plt.legend()
    plt.show()
# --- Main Execution ---
if __name__ == "__main__":
    # Rolling Sharpe Ratio
    plot_rolling_metrics(
        sector_returns,
        sector_returns,
        "Rolling Sharpe Ratio Comparison (Regime-Based vs Equal-Weighted)"
    )
    
    # Compute Sortino Ratio
    sortino_regime = compute_sortino_ratio(sector_returns['Portfolio_Return'])
    sortino_equal = compute_sortino_ratio(sector_returns['Equal_Weighted_Return'])
    print(f"Sortino Ratio - Regime-Based Portfolio: {sortino_regime}")
    print(f"Sortino Ratio - Equal-Weighted Portfolio: {sortino_equal}")
    
    # Compute Maximum Drawdowns
    max_drawdown_regime_based = compute_max_drawdown(sector_returns['Cumulative_Portfolio_Return'])
    max_drawdown_equal_weighted = compute_max_drawdown(sector_returns['Cumulative_Equal_Weighted_Return'])
    
    # Compute Calmar Ratio
    calmar_regime = compute_calmar_ratio(
        sector_returns['Portfolio_Return'],
        max_drawdown_regime_based
    )
    calmar_equal = compute_calmar_ratio(
        sector_returns['Equal_Weighted_Return'],
        max_drawdown_equal_weighted
    )
    print(f"Calmar Ratio - Regime-Based Portfolio: {calmar_regime}")
    print(f"Calmar Ratio - Equal-Weighted Portfolio: {calmar_equal}")
    
    # Stress Testing
    subperiod_results = backtest_by_subperiods(sector_returns)
    print("Subperiod Performance:")
    print(subperiod_results)







